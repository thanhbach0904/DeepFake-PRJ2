{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(DeepfakeDetector, self).__init__()\n",
    "        # Using the smaller EfficientNet-B0 as my training device does not have enough VRAM\n",
    "        if pretrained:\n",
    "            self.efficientnet = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            self.efficientnet = efficientnet_b0(weights=None)\n",
    "\n",
    "        #get feature extractor part of the pretrained model\n",
    "        self.features = self.efficientnet.features\n",
    "        \n",
    "        #modified classifier head since the pretrained one was used to predict 1000 classes\n",
    "        in_features = self.efficientnet.classifier[1].in_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1) #here we use only 1 output neuron because it was enough for binary classificaition, also it fit with the BCELoss later\n",
    "        )\n",
    "        # Replace the original classifier - because it was supposed to used in image classification\n",
    "        self.efficientnet.classifier = self.classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.efficientnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def freeze_features(self):\n",
    "        \"\"\"Freezes the weights of the feature extractor.\"\"\"\n",
    "        print(\"Freezing feature extractor layers...\")\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_features(self):\n",
    "        \"\"\"Unfreezes the weights of the feature extractor.\"\"\"\n",
    "        print(\"Unfreezing all layers...\")\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_dir, num_frames=16, transform=None, split='train', test_size=0.2, random_state=42):\n",
    "        \n",
    "        # data_dir: Root directory containing DFD_manipulated_sequences and DFD_original_sequences folders\n",
    "        # num_frames: Number of frames to extract from each video\n",
    "        # transform: Transform to apply to frames\n",
    "        # split: 'train' or 'test'/'val' to specify which split to use\n",
    "        # test_size: Proportion of data to use for testing (default 0.2 for 20%)\n",
    "        # random_state: Random seed for reproducible splits\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.samples = self._make_dataset(test_size, random_state)\n",
    "\n",
    "    def _make_dataset(self, test_size, random_state):\n",
    "        all_samples = []\n",
    "        \n",
    "        \n",
    "        folder_to_class = {\n",
    "            'DFD_original_sequences': 0,  # real\n",
    "            'DFD_manipulated_sequences': 1  # fake\n",
    "        }\n",
    "        \n",
    "        for folder_name, target_class in folder_to_class.items():\n",
    "            class_dir = os.path.join(self.data_dir, folder_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                print(f\"Warning: Directory {class_dir} does not exist\")\n",
    "                continue\n",
    "                \n",
    "            #Get all video files \n",
    "            video_files = []\n",
    "            for video_name in os.listdir(class_dir):\n",
    "                video_path = os.path.join(class_dir, video_name)\n",
    "                if os.path.isfile(video_path):  \n",
    "                    video_files.append((video_path, target_class))\n",
    "            \n",
    "            \n",
    "            if len(video_files) > 0:\n",
    "                train_files, test_files = train_test_split(\n",
    "                    video_files, \n",
    "                    test_size=test_size, \n",
    "                    random_state=random_state,\n",
    "                    stratify=None  \n",
    "                )\n",
    "                \n",
    "                if self.split == 'train':\n",
    "                    all_samples.extend(train_files)\n",
    "                else:  # test\n",
    "                    all_samples.extend(test_files)\n",
    "                    \n",
    "                print(f\"Class {folder_name}: {len(train_files)} train, {len(test_files)} test samples\")\n",
    "        \n",
    "        print(f\"Total {self.split} samples: {len(all_samples)}\")\n",
    "        return all_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.samples[idx]\n",
    "        \n",
    "        #extract frames from the video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        \n",
    "        if total_frames == 0:\n",
    "            #Replace with a dummy tensor if video is invalid\n",
    "            return torch.zeros((self.num_frames, 3, 224, 224)), -1\n",
    "\n",
    "        frame_indices = sorted(random.sample(range(total_frames), min(self.num_frames, total_frames)))\n",
    "        \n",
    "        for i in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                if self.transform:\n",
    "                    frame = self.transform(frame)\n",
    "                frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "        # If not enough frames were extracted, pad with zeros\n",
    "        if len(frames) < self.num_frames:\n",
    "            num_padding = self.num_frames - len(frames)\n",
    "            padding = torch.zeros((num_padding, 3, 224, 224))\n",
    "            if len(frames) > 0:\n",
    "                frames = torch.stack(frames)\n",
    "                frames = torch.cat((frames, padding), dim=0)\n",
    "            else:\n",
    "                frames = padding\n",
    "        else:\n",
    "            frames = torch.stack(frames)\n",
    "\n",
    "        return frames, label\n",
    "\n",
    "#data transformation will be applied in each frame extracted\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Class DFD_original_sequences: 291 train, 73 test samples\n",
      "Class DFD_manipulated_sequences: 2454 train, 614 test samples\n",
      "Total train samples: 2745\n",
      "Class DFD_original_sequences: 291 train, 73 test samples\n",
      "Class DFD_manipulated_sequences: 2454 train, 614 test samples\n",
      "Total test samples: 687\n",
      "\n",
      "==================================================\n",
      "PHASE 1: Training the classifier head\n",
      "==================================================\n",
      "\n",
      "Freezing feature extractor layers...\n",
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [58:27<00:00, 10.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3567 Acc: 0.8925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 86/86 [14:29<00:00, 10.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3456 Acc: 0.8937\n",
      "New best validation accuracy: 0.8937\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [58:34<00:00, 10.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3361 Acc: 0.8947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 86/86 [14:31<00:00, 10.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3366 Acc: 0.8967\n",
      "New best validation accuracy: 0.8967\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [58:31<00:00, 10.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3233 Acc: 0.8991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 86/86 [14:26<00:00, 10.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3509 Acc: 0.8908\n",
      "\n",
      "Best model saved to deepfake_detector_best.pth with validation accuracy: 0.8967\n",
      "\n",
      "==================================================\n",
      "PHASE 2: Fine-tuning the entire model\n",
      "==================================================\n",
      "\n",
      "Unfreezing all layers...\n",
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [1:07:39<00:00, 11.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3199 Acc: 0.9031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 86/86 [14:57<00:00, 10.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3313 Acc: 0.9083\n",
      "New best validation accuracy: 0.9083\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [1:07:42<00:00, 11.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2915 Acc: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 86/86 [15:01<00:00, 10.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3456 Acc: 0.9025\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [1:07:30<00:00, 11.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2881 Acc: 0.9074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 86/86 [14:55<00:00, 10.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3415 Acc: 0.9127\n",
      "New best validation accuracy: 0.9127\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [1:07:24<00:00, 11.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2793 Acc: 0.9114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 86/86 [14:56<00:00, 10.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3475 Acc: 0.9068\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 344/344 [1:29:35<00:00, 15.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2612 Acc: 0.9129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 86/86 [19:39<00:00, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.3486 Acc: 0.9098\n",
      "\n",
      "Best model saved to deepfake_detector_best.pth with validation accuracy: 0.9127\n",
      "Training complete. Final model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device='cuda'):\n",
    "\n",
    "    # model: The model to validate\n",
    "    # val_loader: DataLoader for validation data\n",
    "    # criterion: Loss function (BCEWithLogitsLoss)\n",
    "    # device: Device to run validation on\n",
    "    \n",
    "    #Returns format: tuple: (validation_loss, validation_accuracy)\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for videos, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "            if -1 in labels:\n",
    "                continue\n",
    "            \n",
    "            videos = videos.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            # (batch_size, num_frames, C, H, W) -> (batch_size * num_frames, C, H, W)\n",
    "            bs, nf, c, h, w = videos.shape\n",
    "            videos = videos.view(-1, c, h, w)\n",
    "            \n",
    "\n",
    "            outputs = model(videos)\n",
    "            outputs = outputs.view(bs, nf).mean(dim=1)  # get average predictions over frames\n",
    "            \n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            #change boolean predictions to float for comparison\n",
    "            preds = preds.float()\n",
    "            \n",
    "            running_loss += loss.item() * bs\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += bs\n",
    "\n",
    "    val_loss = running_loss / total_samples\n",
    "    val_acc = running_corrects.double() / total_samples\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10, device='cuda'):\n",
    "    model.to(device)\n",
    "    \n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = model.state_dict().copy()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for videos, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            if -1 in labels:\n",
    "                continue\n",
    "            \n",
    "            videos = videos.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            \n",
    "            # (batch_size, num_frames, C, H, W) -> (batch_size * num_frames, C, H, W)\n",
    "            bs, nf, c, h, w = videos.shape\n",
    "            videos = videos.view(-1, c, h, w)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(videos)\n",
    "                outputs = outputs.view(bs, nf).mean(dim=1) \n",
    "                \n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                #Calculate predictions for accuracy (apply sigmoid to logits)\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                preds = preds.float()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * bs\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_train_samples += bs\n",
    "\n",
    "        train_loss = running_loss / total_train_samples\n",
    "        train_acc = running_corrects.double() / total_train_samples\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "\n",
    "        #Validation\n",
    "        val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n",
    "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        \n",
    "        #Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            print(f'New best validation accuracy: {best_val_acc:.4f}')\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        print()\n",
    "\n",
    "    #Load best weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    torch.save(model.state_dict(), 'deepfake_detector_best.pth')\n",
    "    print(f\"Best model saved to deepfake_detector_best.pth with validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_FRAMES_PER_VIDEO = 16\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = DeepfakeDetector(pretrained=True)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "    \n",
    "    train_dataset = VideoDataset(data_dir='video_detection_dataset', split='train', transform=data_transforms['train'])\n",
    "    val_dataset = VideoDataset(data_dir='video_detection_dataset', split='test', transform=data_transforms['val'])\n",
    "    \n",
    "    # Use a custom collate function to filter out invalid samples\n",
    "    def collate_fn(batch):\n",
    "        batch = list(filter(lambda x: x[1] != -1, batch))\n",
    "        return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=0, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=0, collate_fn=collate_fn)\n",
    "    # ====================================================================\n",
    "    # Phase 1: Feature Extraction\n",
    "    # ====================================================================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 1: Training the classifier head\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    model.freeze_features()\n",
    "    model.to(device) \n",
    "\n",
    "    \n",
    "    optimizer_phase1 = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=0.001\n",
    "    )\n",
    "    # Train only the head for a few epochs\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer_phase1, None, num_epochs=3, device=device)\n",
    "\n",
    "    # ====================================================================\n",
    "    # Phase 2: Fine-Tuning\n",
    "    # ====================================================================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 2: Fine-tuning the entire model\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    model.unfreeze_features()\n",
    "\n",
    "    # Create a new optimizer for the whole model with a very low learning rate\n",
    "    optimizer_phase2 = optim.Adam(model.parameters(), lr=0.00005) \n",
    "    # This method is called adaptive learning.\n",
    "    scheduler = StepLR(optimizer_phase2, step_size=4, gamma=0.1)\n",
    "    \n",
    "\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer_phase2, scheduler, num_epochs=5, device=device)\n",
    "    print(\"Training complete. Final model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
